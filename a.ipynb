{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Visual Search\n",
    "In many domains we are interested in finding artifacts that are similar to a query artifact. In this assignment you are going to implement a system that can find artifacts when the queries are visual.\n",
    "\n",
    "The type of queries you will test your system on are MS COCO dataset images.\n",
    "\n",
    "# 1. Dataset\n",
    "Your images will be from the MSCOCO mini dataset and will be of class person. Download the dataset to your Gdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ann_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./instances_minitrain2017.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m coco\u001b[39m=\u001b[39mCOCO(ann_file)\n\u001b[1;32m      7\u001b[0m imgIds \u001b[39m=\u001b[39m coco\u001b[39m.\u001b[39mgetImgIds(catIds\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m])\n\u001b[1;32m      8\u001b[0m images \u001b[39m=\u001b[39m coco\u001b[39m.\u001b[39mloadImgs(imgIds)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pycocotools/coco.py:82\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     80\u001b[0m tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     81\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(annotation_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 82\u001b[0m     dataset \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m     83\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(dataset)\u001b[39m==\u001b[39m\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mannotation file format \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not supported\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(dataset))\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone (t=\u001b[39m\u001b[39m{:0.2f}\u001b[39;00m\u001b[39ms)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39m tic))\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m     \u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[39mdel\u001b[39;00m kw[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m     \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "\n",
    "ann_file = \"./instances_minitrain2017.json\"\n",
    "coco=COCO(ann_file)\n",
    "\n",
    "imgIds = coco.getImgIds(catIds=[1])\n",
    "images = coco.loadImgs(imgIds)\n",
    "\n",
    "# for im in images:\n",
    "#     img_data = requests.get(im['coco_url']).content\n",
    "#     with open('./coco_person/' + im['file_name'], 'wb') as handler:\n",
    "#         handler.write(img_data)\n",
    "print(len(set(imgIds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pretrained Object Detection\n",
    "Using a model pretrained to perform object detection on MS COCO (e.g. https://pytorch.org/vision/stable/models/faster_rcnn.html ) extract the bounding boxes of all people in your dataset. List details of the object detector used and code the pipeline that results in the required inference json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.5 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Program Files (x86)/Microsoft Visual Studio/Shared/Python37_64/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "\n",
    "ann_file = \"./instances_minitrain2017.json\"\n",
    "coco=COCO(ann_file)\n",
    "\n",
    "imgIds = coco.getImgIds(catIds=[1])\n",
    "images = coco.loadImgs(imgIds)\n",
    "\n",
    "print(images[0])\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "# img = Image.open(\"./coco_person/000000000036.jpg\")\n",
    "# convert_tensor = transforms.ToTensor()\n",
    "# tensor = convert_tensor(img)\n",
    "# predict = model(coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import detection\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", type=str, required=True,\n",
    "\thelp=\"path to the input image\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"frcnn-resnet\",\n",
    "\tchoices=[\"frcnn-resnet\", \"frcnn-mobilenet\", \"retinanet\"],\n",
    "\thelp=\"name of the object detection model\")\n",
    "ap.add_argument(\"-l\", \"--labels\", type=str, default=\"coco_classes.pickle\",\n",
    "\thelp=\"path to file containing list of categories in COCO dataset\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "\thelp=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# set the device we will be using to run the model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load the list of categories in the COCO dataset and then generate a\n",
    "# set of bounding box colors for each class\n",
    "CLASSES = pickle.loads(open(args[\"labels\"], \"rb\").read())\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "# initialize a dictionary containing model name and its corresponding \n",
    "# torchvision function call\n",
    "MODELS = {\n",
    "\t\"frcnn-resnet\": detection.fasterrcnn_resnet50_fpn,\n",
    "\t\"frcnn-mobilenet\": detection.fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "\t\"retinanet\": detection.retinanet_resnet50_fpn\n",
    "}\n",
    "\n",
    "# load the model and set it to evaluation mode\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "model.eval()\n",
    "\n",
    "# load the image from disk\n",
    "image = cv2.imread(args[\"image\"])\n",
    "orig = image.copy()\n",
    "# convert the image from BGR to RGB channel ordering and change the\n",
    "# image from channels last to channels first ordering\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image.transpose((2, 0, 1))\n",
    "# add the batch dimension, scale the raw pixel intensities to the\n",
    "# range [0, 1], and convert the image to a floating point tensor\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = image / 255.0\n",
    "image = torch.FloatTensor(image)\n",
    "# send the input to the device and pass the it through the network to\n",
    "# get the detections and predictions\n",
    "image = image.to(DEVICE)\n",
    "detections = model(image)[0]\n",
    "\n",
    "# loop over the detections\n",
    "for i in range(0, len(detections[\"boxes\"])):\n",
    "\t# extract the confidence (i.e., probability) associated with the\n",
    "\t# prediction\n",
    "\tconfidence = detections[\"scores\"][i]\n",
    "\t# filter out weak detections by ensuring the confidence is\n",
    "\t# greater than the minimum confidence\n",
    "\tif confidence > args[\"confidence\"]:\n",
    "\t\t# extract the index of the class label from the detections,\n",
    "\t\t# then compute the (x, y)-coordinates of the bounding box\n",
    "\t\t# for the object\n",
    "\t\tidx = int(detections[\"labels\"][i])\n",
    "\t\tbox = detections[\"boxes\"][i].detach().cpu().numpy()\n",
    "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\t# display the prediction to our terminal\n",
    "\t\tlabel = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
    "\t\tprint(\"[INFO] {}\".format(label))\n",
    "\t\t# draw the bounding box and label on the image\n",
    "\t\tcv2.rectangle(orig, (startX, startY), (endX, endY),\n",
    "\t\t\tCOLORS[idx], 2)\n",
    "\t\ty = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "\t\tcv2.putText(orig, label, (startX, y),\n",
    "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "# show the output image\n",
    "cv2.imshow(\"Output\", orig)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
