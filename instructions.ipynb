{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Visual Search\n",
    "In many domains we are interested in finding artifacts that are similar to a query artifact. In this assignment you are going to implement a system that can find artifacts when the queries are visual.\n",
    "\n",
    "The type of queries you will test your system on are MS COCO dataset images.\n",
    "\n",
    "# 1. Dataset\n",
    "Your images will be from the MSCOCO mini dataset and will be of class person. Download the dataset to your Gdrive.\n",
    "\n",
    "**For this part I dowloaded all the person images localy to coco_person folser by running dowmload-images.py. For this I used instances_minitrain2017.json that was given in the question. (both coco_person and instances_minitrain2017.json are in gitignore because the're too large.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pretrained Object Detection\n",
    "Using a model pretrained to perform object detection on MS COCO (e.g. https://pytorch.org/vision/stable/models/faster_rcnn.html ) extract the bounding boxes of all people in your dataset. List details of the object detector used and code the pipeline that results in the required inference json file.\n",
    "\n",
    "**For this part I ran detect.sh to detect all the boxes of the person class using faster rcnn using 80% confidence. Then wrote all the data to person_boxes.json. Then ran crop.py to crop the images using the json file and output it into boxes_images folder. (this folder is in gitignore because it's too large)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reverse Image Search - Feature extraction\n",
    "See this example on how AWS is suggesting to build a reverse image search system.\n",
    "\n",
    "Using eg. ResNet-50 as a featurizer (ensure that is the featurizer used by the object detector), extract for each detected bounding box the representation vector (2048 elements long).\n",
    "\n",
    "**For this part I ran feature-extraction.py and wrote into features.npy file (this file is in gitignore because it's too large)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reverse Image Search - Similarity Search (30 points)\n",
    "Using five (5) of the person bounding boxes as query, pick a similarity search python library to return the 10 most similar bounded boxed persons to the query image.\n",
    "\n",
    "Before you can execute the similarity search, you may have to do dimensionality reduction - clearly explain how you did that.\n",
    "\n",
    "Select carefully the query image so that the results can be revealing of the ability of your representation and the search engine.\n",
    "\n",
    "**I picked PyNNDescent as the similarity search pytohn library. I ran similarity-search.py to find the 10 most similar boxes 58, 24, 27, 30, and 31. The output is on similarity-results.png. For this algorithm I used the features I extracted in the last step as dimensionality reduction.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
